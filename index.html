<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="StyleganFusion, image generator domain adaptation. StyleGAN, StyleGAN-NADA, StableDiffusion, Diffusion, Score distillation Sampling, DreamFusion">
  <meta name="keywords" content=" Synthesis, Multimodal, VQGAN">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>StyleGAN-Fusion: Diffusion Guided Image Generator Domain Adaptation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
<!--   <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');



  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <!-- <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://phymhan.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a> -->

      <!-- <div class="navbar-item has-dropdown is-hoverable"> -->
        <!-- <a class="navbar-link">
          More Research
        </a> -->
        <!-- <div class="navbar-dropdown">
          <a class="navbar-item" href="https://bluer555.github.io/MoCoGAN-HD/">
            MoCoGAN-HD
          </a>
        </div>
      </div> -->
    <!-- </div>

  </div> -->
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Diffusion Guided Image Generator Domain Adaptation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://kunpengsong.github.io/">Kunpeng Song</a><sup>1</sup>
            </span>
            <span class="author-block">
              <a href="https://phymhan.github.io">Ligong Han</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="">Bingchen Liu</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="">Ahmed Elgammal</a><sup>1</sup>,
            </span>
            <!-- <span class="author-block">
              <a href="https://fvancesco.github.io">Francesco Barbieri</a><sup>2</sup>,
            </span> -->
          </div>
          <!-- <h1 style="font-size:23px;font-weight:bold"></h1> -->

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Rutgers University,</span>
            <span class="author-block"><sup>2</sup>Bytedance Inc.</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://www.dropbox.com/s/ge8zlvd6zblnexc/mmvid_full.pdf?dl=0"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://www.dropbox.com/s/ge8zlvd6zblnexc/mmvid_full.pdf?dl=0"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/snap-research/MMVID"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/snap-research/MMVID"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <video id="teaser" autoplay muted loop height="100%">
        <source src="static/videos/main.mp4"
                type="video/mp4">
      </video> -->
      <!-- <img src="static/videos/demo.gif"> -->
      <img src="static/images/Hero_image-1.png">
      <h2 class="subtitle has-text-centered">
        Generated images after adapting FFHQ/AFHQ-cat generators to a 3D rendering style.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Can a text-to-image diffusion model be used as a training objective for adapting a GAN generator to another domain? In this paper, we show that the classifier-free guidance can be leveraged as a critic and enable generators to distill knowledge from large-scale text-to-image diffusion models. Generators can be efficiently shifted into new domains indicated by text prompts without access to groundtruth samples from target domains. We demonstrate the effectiveness and controllability of our method through extensive experiments. Although not trained to minimize CLIP loss, our model achieves equally high CLIP scores and significantly lower FID than prior work on short prompts, and outperforms the baseline qualitatively and quantitatively on long and complicated prompts. To our best knowledge, the proposed method is the first attempt at incorporating large-scale pre-trained diffusion models and distillation sampling for text-driven image generator domain adaptation and gives a quality previously beyond possible. Moreover, we extend our work to 3D-aware style-based generators.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Model Overview</h2>
        <div class="content has-text-centered">
          <img src="static/images/pipeline_v2-1.png">
          <p>
            Uncrated samples from our method on Cat-to-Aanimals.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- 
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Experiment</h2>
        <div class="content has-text-centered">
        <h2 class="title is-3">AFHQ-cat to Animals</h2>
        <div class="content has-text-justified">
          <p>
            Samples generated by our approach on the Moving Shapes dataset for text-to-video generation. We show three synthesized videos for each input text condition.
          </p>
        </div>
        <div class="content has-text-centered">
          <video class="video-fluid w-100" controls autoplay loop muted>
            <source src="static/videos/shape_text-to-video.mp4" type="video/mp4" />
          </video>
        </div>
        <h3 class="title is-4">Independent multimodal video generation</h3>
        <div class="content has-text-justified">
          <p>
            Samples generated by our approach on the Shapes dataset for independent multimodal generation. The input control signals are text and a partially observed image (with the center masked out, shown in white color). We show two synthesized videos for each input multimodal condition.
          </p>
        </div>
        <div class="content has-text-centered">
          <video class="video-fluid w-100" controls autoplay loop muted>
            <source src="static/videos/shape_text+ic.mp4" type="video/mp4" />
          </video>
        </div>
        <h3 class="title is-4">Dependent multimodal video generation</h3>
        <div class="content has-text-justified">
          <p>
            Samples generated by our approach on the Shapes dataset for dependent multimodal generation. The input control signals are text and images. We show one synthesized video for each input multimodal condition.
          </p>
        </div>
        <div class="content has-text-centered">
          <video class="video-fluid w-100" controls autoplay loop muted>
            <source src="static/videos/shape_depend.mp4" type="video/mp4" />
          </video>
        </div>
        <h2 class="title is-3">iPER Dataset</h2>
        <h3 class="title is-4">Long sequence generation</h3>
        <div class="content has-text-justified">
          <p>
            Example videos generated by our approach on the iPER dataset for long sequence generation. The extrapolation process is repeated for each sequence 100 times, resulting in a 107-frame video. The textual input also controls the speed, where "slow" indicates videos with slow speed such that the motion is slow, while "fast" indicates the performed motion is fast. We show one synthesized video for each input text condition. The first video following the text input corresponds to the "slow" condition, the second corresponds to the "normal", and the last corresponds to the "fast".
          </p>
        </div>
        <div class="content has-text-centered">
          <video class="video-fluid w-100" controls autoplay loop muted>
            <source src="static/videos/iper_long.mp4" type="video/mp4" />
          </video>
        </div>
        <h3 class="title is-4">Temporal Interpolation</h3>
        <div class="content has-text-justified">
          <p>
            Example videos of our approach for video interpolation on iPER dataset.
          </p>
        </div>
        <div class="content has-text-centered">
          <video class="video-fluid w-100" controls autoplay loop muted>
            <source src="static/videos/iper_interp.mp4" type="video/mp4" />
          </video>
        </div>
      <br></br>
        <h2 class="title is-3">Supplemental Materials</h2>
        <div class="content has-text-justified">
          <p>
            More supplemental videos can be found at this <a href="https://phymhan.github.io/sites/mmvid_supp/">webpage</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</section> -->

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{han2022mmvid,
  author    = {Han, Ligong and Ren, Jian and Lee, Hsin-Ying and Barbieri, Francesco and Olszewski, Kyle and Minaee, Shervin and Metaxas, Dimitris and Tulyakov, Sergey},
  title     = {Show Me What and Tell Me How: Video Synthesis via Multimodal Conditioning},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year      = {2022},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://www.dropbox.com/s/ge8zlvd6zblnexc/mmvid_full.pdf?dl=0">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/phymhan" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
